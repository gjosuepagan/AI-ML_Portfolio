{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RND7vr_TvkKM",
        "Rl7TaXhkvGqG",
        "YFfc4iyQvKyq",
        "3leRuE0ivP-0",
        "145v9_FzL9I-",
        "6H2oc_4MMCTI",
        "HIXZP9aAM400",
        "reBaXdb2wP_5",
        "7vROzT-8NOzp",
        "dh1jBJp0wzKr",
        "TbHteOIVNgAr",
        "4dMMvm3ExOss",
        "XjjjW_jpxS31",
        "5qVmHwoUriUP",
        "o5kCJPv2rtRf",
        "54wLjKoAwfjf",
        "eOLKRUllwnFo",
        "3PjJ3HUawrsM",
        "szWpemB9x9Vd",
        "05dhLGnNyEmt",
        "p72gLCYOyv9l",
        "pg46s9Bi9LZi",
        "S4JLebxA9yK7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Computer Vision: COVID-19 Image Classification"
      ],
      "metadata": {
        "id": "Q_QihvwPu-9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement"
      ],
      "metadata": {
        "id": "RND7vr_TvkKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Context"
      ],
      "metadata": {
        "id": "Rl7TaXhkvGqG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wAinItq04rv"
      },
      "source": [
        "Covid-19 is a fast-growing disease that affects human health severly. Patients diagonised with this condition suffers from lung infection. The medical community has recently released vaccines which have a slower effect in increasing the immunity. This virus has impacted various countries' human health and financial standing.  <br>\n",
        "\n",
        "Deep learning algorithms have recently used image classification to identify medical images. Convolutional Neural Networks (CNN) can be widely utilized to identify COVID-19 to assist radiologists in medical analysis by classifying patients who are healthy, have viral pneumonia, or are affected by COVID using X-ray pictures of the lungs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective"
      ],
      "metadata": {
        "id": "YFfc4iyQvKyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this project is to **Build a Convolutional Neural Network to differentiate an X-ray image of a person affected with covid from that of a healthy person or a person who has viral pneumonia(fever).**"
      ],
      "metadata": {
        "id": "6qYOStX8vNGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Dictionary"
      ],
      "metadata": {
        "id": "3leRuE0ivP-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This dataset contains training set images of 3 classes which are converted into numpy arrays.\n",
        "\n",
        "- The dataset comprises 3 classes:\n",
        "  - COVID-19: The patient who is effected due to covid.\n",
        "  - Viral Pneumonia: This is a viral fever which has similar characteristics like fever and cought that of Covid but is not covid.\n",
        "  - Normal- A healthy Person with no symptoms of covid or fever.\n",
        "\n",
        "- The data file names are:\n",
        "  - CovidImages.npy\n",
        "  - CovidLabels.csv"
      ],
      "metadata": {
        "id": "DpTeoUzHvV88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Note: Please use GPU runtime to execute the code efficiently**"
      ],
      "metadata": {
        "id": "_1nl1Nv_vfLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Please read the instructions carefully before starting the project.**\n",
        "\n",
        "This is a commented Python Notebook file in which all the instructions and tasks to be performed are mentioned.\n",
        "\n",
        "* Blanks '_______' are provided in the notebook that need to be filled with an appropriate code to get the correct result\n",
        "\n",
        "* With every '_______' blank, there is a comment that briefly describes what needs to be filled in the blank space\n",
        "\n",
        "* Identify the task to be performed correctly and only then proceed to write the required code\n",
        "\n",
        "* Fill the code wherever asked by the commented lines like \"# write your code here\" or \"# complete the code\"\n",
        "\n",
        "* Running incomplete code may throw an error\n",
        "\n",
        "* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors\n",
        "\n",
        "* Add the results/observations derived from the analysis in the presentation and submit the same in .pdf format"
      ],
      "metadata": {
        "id": "tdjWvNznv_oJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary libraries"
      ],
      "metadata": {
        "id": "145v9_FzL9I-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvez2h70T54x"
      },
      "source": [
        "import os\n",
        "import numpy as np                                                                               # Importing numpy for Matrix Operations\n",
        "import pandas as pd                                                                              # Importing pandas to read CSV files\n",
        "import matplotlib.pyplot as plt                                                                  # Importting matplotlib for Plotting and visualizing images\n",
        "import math                                                                                      # Importing math module to perform mathematical operations\n",
        "import cv2                                                                                       # Importing openCV for image processing\n",
        "import seaborn as sns                                                                            # Importing seaborn to plot graphs\n",
        "\n",
        "\n",
        "# Tensorflow modules\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator                              # Importing the ImageDataGenerator for data augmentation\n",
        "from tensorflow.keras.models import Sequential                                                   # Importing the sequential module to define a sequential model\n",
        "from tensorflow.keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPooling2D,BatchNormalization # Defining all the layers to build our CNN Model\n",
        "from tensorflow.keras.optimizers import Adam,SGD                                                 # Importing the optimizers which can be used in our model\n",
        "from sklearn import preprocessing                                                                # Importing the preprocessing module to preprocess the data\n",
        "from sklearn.model_selection import train_test_split                                             # Importing train_test_split function to split the data into train and test\n",
        "from sklearn.metrics import confusion_matrix                                                     # Importing confusion_matrix to plot the confusion matrix\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "# Display images using OpenCV\n",
        "from google.colab.patches import cv2_imshow                                                      # Importing cv2_imshow from google.patches to display images\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import backend\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import random\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset"
      ],
      "metadata": {
        "id": "6H2oc_4MMCTI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOSarXG3Uyw6"
      },
      "source": [
        "# Mount Google drive to access the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWB8yf6vVRnZ"
      },
      "source": [
        "# Load the image file of dataset\n",
        "images = np.load('______')      # Complete the code to read the dataset\n",
        "\n",
        "# Load the labels file of dataset\n",
        "labels = pd.read_csv('______')  # Complete the code to read the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Overview"
      ],
      "metadata": {
        "id": "HIXZP9aAM400"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understand the shape of the dataset"
      ],
      "metadata": {
        "id": "reBaXdb2wP_5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jXF3mY7VpOQ"
      },
      "source": [
        "print(______.shape)         # Complete the code to check the shape of images\n",
        "print(______.shape)         # Complete the code to check the shape of labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "7vROzT-8NOzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting random images from each of the class"
      ],
      "metadata": {
        "id": "dh1jBJp0wzKr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6cu7BcvWdGC"
      },
      "source": [
        "def plot_images(images,labels):\n",
        "  num_classes=10                                                                  # Number of Classes\n",
        "  categories=np.unique(labels)\n",
        "  keys=dict(labels['Label'])                                                      # Obtaing the unique classes from y_train\n",
        "  rows = 3                                                                        # Defining number of rows=3\n",
        "  cols = 4                                                                        # Defining number of columns=4\n",
        "  fig = plt.figure(figsize=(10, 8))                                               # Defining the figure size to 10x8\n",
        "  for i in range(cols):\n",
        "      for j in range(rows):\n",
        "          random_index = np.random.randint(0, len(labels))                        # Generating random indices from the data and plotting the images\n",
        "          ax = fig.add_subplot(rows, cols, i * rows + j + 1)                      # Adding subplots with 3 rows and 4 columns\n",
        "          ax.imshow(images[random_index, :])                                      # Plotting the image\n",
        "          ax.set_title(keys[random_index])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G8dnqi0Wj4R"
      },
      "source": [
        "plot_images(______,______)   # Complete the code to input the images and labels to the function and plot the images with their labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the distribution of the target variable"
      ],
      "metadata": {
        "id": "TbHteOIVNgAr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2cwRvCyj2ok"
      },
      "source": [
        "sns.countplot(x=labels['______'])            # Complete the code to check for data imbalance\n",
        "plt.xticks(rotation='vertical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pre-Processing"
      ],
      "metadata": {
        "id": "4dMMvm3ExOss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting the BGR images to RGB images."
      ],
      "metadata": {
        "id": "XjjjW_jpxS31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the images from BGR to RGB using cvtColor function of OpenCV\n",
        "for i in range(len(images)):\n",
        "  images[i] = cv2.cvtColor(images[i], cv2.______)        # Complete the code to convert the images from BGR to RGB"
      ],
      "metadata": {
        "id": "7RpxSK3ExdSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resizing images"
      ],
      "metadata": {
        "id": "5qVmHwoUriUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the size of the images is large, it may be computationally expensive to train on these larger images; therefore, it is preferable to reduce the image size from 128 to 64."
      ],
      "metadata": {
        "id": "rKE_vKRjxkeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images_decreased=[]\n",
        "height = ______                    # Complete the code to define the height as 64\n",
        "width =  ______                    # Complete the code to define the width as 64\n",
        "dimensions = (width, height)\n",
        "for i in range(len(images)):\n",
        "  images_decreased.append( cv2.resize(images[i], dimensions, interpolation=cv2.INTER_LINEAR))"
      ],
      "metadata": {
        "id": "o--ysjNMxpnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image before resizing**"
      ],
      "metadata": {
        "id": "5lsTWu7xxtrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(images[3])"
      ],
      "metadata": {
        "id": "gFar1OcTrle7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image after resizing**"
      ],
      "metadata": {
        "id": "RrAhMfoIxw0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(images_decreased[3])"
      ],
      "metadata": {
        "id": "jJ3JAVJ2x0ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation for Modeling"
      ],
      "metadata": {
        "id": "o5kCJPv2rtRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As we have less images in our dataset, we will only use 10% of our data for testing, 10% of our data for validation and 80% of our data for training.\n",
        "- We are using the train_test_split() function from scikit-learn. Here, we split the dataset into three parts, train,test and validation."
      ],
      "metadata": {
        "id": "lQ6QXe8Kx-6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(np.array(images_decreased),____ , test_size=0.1, random_state=42,stratify=labels) # Complete the code to split the data with test_size as 0.1"
      ],
      "metadata": {
        "id": "RhCeqk_Urlhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding the target labels"
      ],
      "metadata": {
        "id": "54wLjKoAwfjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels from names to one hot vectors.\n",
        "# We have already used encoding methods like onehotencoder and labelencoder earlier so now we will be using a new encoding method called labelBinarizer.\n",
        "# Labelbinarizer works similar to onehotencoder\n",
        "\n",
        "enc = ______                                        # Complete the code to intialize the labelBinarizer\n",
        "y_train_encoded = enc.fit_transform(______)         # Complete the code to fit and transform y_train\n",
        "y_test_encoded=enc.transform(______)                # Complete the code to transform y_test"
      ],
      "metadata": {
        "id": "P30HqWTIrlnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_encoded.shape\n",
        "______.shape    # Complete the code to check the shape of y_test data"
      ],
      "metadata": {
        "id": "K9qaxl0T3sab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Normalization"
      ],
      "metadata": {
        "id": "eOLKRUllwnFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the **image pixel values range from 0-255**, our method of normalization here will be **scaling** - we shall **divide all the pixel values by 255 to standardize the images to have values between 0-1.**"
      ],
      "metadata": {
        "id": "JjdrmKWH3_Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the code to normalize the image pixels of train, and test\n",
        "train_normalized = ______.astype('float32')/255.0\n",
        "X_test_normalized = ______.astype('float32')/255.0"
      ],
      "metadata": {
        "id": "Z6Ngpdrh4EkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building"
      ],
      "metadata": {
        "id": "3PjJ3HUawrsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clearing backend\n",
        "backend.clear_session()"
      ],
      "metadata": {
        "id": "XmepBpUxwjHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixing the seed for random number generators\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "ncE9VzyZ6eMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Intializing a sequential model\n",
        "model1 = ______                             # Complete the code to intialize a sequential model\n",
        "\n",
        "# Complete the code to add the first conv layer with 128 filters and kernel size 3x3 , padding 'same' provides the output size same as the input size\n",
        "# Input_shape denotes input image dimension of images\n",
        "model1.add(Conv2D(______, (3, 3), activation='______', padding=\"same\", input_shape=(64, 64, 3)))\n",
        "\n",
        "# Complete the code to add the max pooling to reduce the size of output of first conv layer\n",
        "model1.add(______((2, 2), padding = 'same'))\n",
        "\n",
        "# Complete the code to create two similar convolution and max-pooling layers activation = relu\n",
        "model1.add(Conv2D(64, (3, 3), activation='______', padding=\"same\"))\n",
        "model1.add(______((2, 2), padding = 'same'))\n",
        "\n",
        "model1.add(Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n",
        "model1.add(MaxPooling2D((2, 2), padding = 'same'))\n",
        "\n",
        "# Complete the code to flatten the output of the conv layer after max pooling to make it ready for creating dense connections\n",
        "model1.add(______())\n",
        "\n",
        "# Complete the code to add a fully connected dense layer with 16 neurons\n",
        "model1.add(Dense(______, activation='relu'))\n",
        "model1.add(Dropout(0.3))\n",
        "# Complete the code to add the output layer with 3 neurons and activation functions as softmax since this is a multi-class classification problem\n",
        "model1.add(Dense(______, activation='softmax'))\n",
        "\n",
        "# Complete the code to use the Adam Optimizer\n",
        "opt=Adam()\n",
        "# Complete the code to Compile the model using suitable metric for loss fucntion\n",
        "model1.compile(optimizer=______, loss='______', metrics=['accuracy'])\n",
        "\n",
        "# Complete the code to generate the summary of the model\n",
        "model1.______()"
      ],
      "metadata": {
        "id": "Or-yPGy36uJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Fitting the model on the train data"
      ],
      "metadata": {
        "id": "sHPh8qw5w9Cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the code to fit the model on train data\n",
        "history_1 = model1.fit(\n",
        "            ______, ______,\n",
        "            epochs=30,\n",
        "            validation_split=0.1,\n",
        "            shuffle=False,\n",
        "            batch_size=64,\n",
        "            verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "bmNjd1PEw_Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evaluation**"
      ],
      "metadata": {
        "id": "QsumvPFExBpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_1.history['accuracy'])\n",
        "plt.plot(history_1.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R-Y6K312xECF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the model on test data**"
      ],
      "metadata": {
        "id": "qj0OpAoXxLrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = model1.evaluate(______, ______, verbose=2)    # Complete the code to evaluate the model on test data"
      ],
      "metadata": {
        "id": "gczfuqtZ7Qht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting the Confusion Matrix**"
      ],
      "metadata": {
        "id": "0V-uYSyUxYPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we would get the output as probablities for each category\n",
        "y_pred=model1.predict(______)                          # Complete the code to predict the output probabilities"
      ],
      "metadata": {
        "id": "t4-nNsuD7Xfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtaining the categorical values from y_test_encoded and y_pred\n",
        "y_pred_arg=np.argmax(y_pred,axis=1)\n",
        "y_test_arg=np.argmax(y_test_encoded,axis=1)\n",
        "\n",
        "# Plotting the Confusion Matrix using confusion matrix() function which is also predefined in tensorflow module\n",
        "confusion_matrix = tf.math.confusion_matrix(______,______)              # Complete the code to plot the confusion matrix\n",
        "f, ax = plt.subplots(figsize=(12, 12))\n",
        "sns.heatmap(\n",
        "    confusion_matrix,\n",
        "    annot=True,\n",
        "    linewidths=.4,\n",
        "    fmt=\"d\",\n",
        "    square=True,\n",
        "    ax=ax\n",
        ")\n",
        "# Setting the labels to both the axes\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n",
        "ax.set_title('Confusion Matrix');\n",
        "ax.xaxis.set_ticklabels(['Covid', 'Normal', 'Viral Pneumonia'],rotation=20)\n",
        "ax.yaxis.set_ticklabels(['Covid', 'Normal', 'Viral Pneumonia'],rotation=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ds5e2C7-7bN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting Classification Report**"
      ],
      "metadata": {
        "id": "2egj3AMW708c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the classification report\n",
        "cr=metrics.classification_report(_______,_______)     # Complete the code to plot the classification report\n",
        "print(cr)"
      ],
      "metadata": {
        "id": "mOR4x1pK72mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Performance Improvement"
      ],
      "metadata": {
        "id": "szWpemB9x9Vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reducing the Learning Rate:**\n",
        "\n",
        "**ReduceLRonPlateau()** is a function that will be used to decrease the learning rate by some factor, if the loss is not decreasing for some time. This may start decreasing the loss at a smaller learning rate. There is a possibility that the loss may still not decrease. This may lead to executing the learning rate reduction again in an attempt to achieve a lower loss."
      ],
      "metadata": {
        "id": "z7r2iRivyAKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n",
        "                                            patience=3,\n",
        "                                            verbose=1,\n",
        "                                            factor=0.5,\n",
        "                                            min_lr=0.0001)\n"
      ],
      "metadata": {
        "id": "saKsXNVrxdWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Augmentation**"
      ],
      "metadata": {
        "id": "05dhLGnNyEmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clearing backend\n",
        "from tensorflow.keras import backend\n",
        "backend.clear_session()\n",
        "\n",
        "# Fixing the seed for random number generators\n",
        "import random\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "-ZBGf_aVxdYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All images to be rescaled by 1/255.\n",
        "train_datagen = ImageDataGenerator(\n",
        "                              rotation_range=20,\n",
        "                              fill_mode='nearest'\n",
        "                              )\n",
        "# test_datagen  = ImageDataGenerator(rescale = 1.0/255.)"
      ],
      "metadata": {
        "id": "WRUx1PCpyIZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Intializing a sequential model\n",
        "model2 = Sequential()\n",
        "\n",
        "# Complete the code to add the first conv layer with 64 filters and kernel size 3x3 , padding 'same' provides the output size same as the input size\n",
        "# Input_shape denotes input image dimension images\n",
        "model2.add(______(64, (______, ______), activation='relu', padding=\"same\", input_shape=(64, 64, 3)))\n",
        "\n",
        "# Complete the code to add max pooling to reduce the size of output of first conv layer\n",
        "model2.add(______((2, 2), padding = 'same'))\n",
        "\n",
        "\n",
        "model2.add(Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n",
        "model2.add(MaxPooling2D((2, 2), padding = 'same'))\n",
        "model2.add(BatchNormalization())\n",
        "\n",
        "# flattening the output of the conv layer after max pooling to make it ready for creating dense connections\n",
        "model2.add(Flatten())\n",
        "\n",
        "# Adding a fully connected dense layer with 16 neurons\n",
        "model2.add(Dense(16, activation='relu'))\n",
        "\n",
        "# Complete the code to add dropout with dropout_rate=0.3\n",
        "model2.add(Dropout(______))\n",
        "# Complete the code to add the output layer with 3 neurons and activation functions as softmax since this is a multi-class classification problem\n",
        "model2.add(Dense(___, activation='______'))\n",
        "\n",
        "# Complete the code to initialize Adam Optimimzer\n",
        "opt=______\n",
        "# Complete the code to Compile model\n",
        "model2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Generating the summary of the model\n",
        "model2.______()"
      ],
      "metadata": {
        "id": "31QrxLSUyIcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the code to fit the model on train data with batch_size=64 and epochs=30\n",
        "# Epochs\n",
        "epochs = ______\n",
        "# Batch size\n",
        "batch_size = ______\n",
        "\n",
        "\n",
        "history = model2.fit(train_datagen.flow(X_train_normalized,y_train_encoded,\n",
        "                                       batch_size=batch_size,\n",
        "                                       shuffle=False),\n",
        "                                       epochs=epochs,\n",
        "                                       steps_per_epoch=X_train_normalized.shape[0] // batch_size,\n",
        "                                       validation_data=(X_test_normalized,y_test_encoded),\n",
        "                                       verbose=1,callbacks=[learning_rate_reduction])"
      ],
      "metadata": {
        "id": "UMl3m-JlyIgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evaluation**"
      ],
      "metadata": {
        "id": "YjFLkySA8mg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9UZ4WfhlyIiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the model on test data**"
      ],
      "metadata": {
        "id": "acm5dFQz8pAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = model2.evaluate(______, y_test_encoded, verbose=2)  # Complete the code to evaluate the model on test data"
      ],
      "metadata": {
        "id": "LTwwgOXo8qyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting the Confusion Matrix**"
      ],
      "metadata": {
        "id": "bdV6vVqx8ts-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the code to obtain the output probabilities\n",
        "y_pred=model2.predict(______)"
      ],
      "metadata": {
        "id": "Qh1gyc1T8w5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtaining the categorical values from y_test_encoded and y_pred\n",
        "y_pred_arg=np.argmax(y_pred,axis=1)\n",
        "y_test_arg=np.argmax(y_test_encoded,axis=1)\n",
        "\n",
        "# Plotting the Confusion Matrix using confusion matrix() function which is also predefined in tensorflow module\n",
        "confusion_matrix = tf.math.confusion_matrix(______,y_pred_arg)     # Complete the code to obatin the confusion matrix\n",
        "f, ax = plt.subplots(figsize=(12, 12))\n",
        "sns.heatmap(\n",
        "    confusion_matrix,\n",
        "    annot=True,\n",
        "    linewidths=.4,\n",
        "    fmt=\"d\",\n",
        "    square=True,\n",
        "    ax=ax\n",
        ")\n",
        "# Setting the labels to both the axes\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n",
        "ax.set_title('Confusion Matrix');\n",
        "ax.xaxis.set_ticklabels(['Covid', 'Normal', 'Viral Pneumonia'],rotation=20)\n",
        "ax.yaxis.set_ticklabels(['Covid', 'Normal', 'Viral Pneumonia'],rotation=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vL0sF9bY82lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting Classification Report**"
      ],
      "metadata": {
        "id": "CxdgzU7c89sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the classification report\n",
        "cr=metrics.classification_report(_______,_______)     # Complete the code to plot the classification report\n",
        "print(cr)"
      ],
      "metadata": {
        "id": "Y9AdGpt98_lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Model"
      ],
      "metadata": {
        "id": "p72gLCYOyv9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comment on the final model you have selected and use the same in the below code to visualize the image."
      ],
      "metadata": {
        "id": "NY6qGg6H9IXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing the prediction"
      ],
      "metadata": {
        "id": "pg46s9Bi9LZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the predicted and correct label of images from test data\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(X_test[2])\n",
        "plt.show()\n",
        "## Complete the code to predict the test data using the final model selected\n",
        "print('Predicted Label', enc.inverse_transform(_____.predict((X_test_normalized[2].reshape(1,64,64,3)))))   # reshaping the input image as we are only trying to predict using a single image\n",
        "print('True Label', enc.inverse_transform(y_test_encoded)[2])                                               # using inverse_transform() to get the output label from the output vector\n",
        "\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(X_test[10])\n",
        "plt.show()\n",
        "## Complete the code to predict the test data using the final model selected\n",
        "print('Predicted Label', enc.inverse_transform(_______.predict((X_test_normalized[33].reshape(1,64,64,3)))))  # reshaping the input image as we are only trying to predict using a single image\n",
        "print('True Label', enc.inverse_transform(y_test_encoded)[10])                                              # using inverse_transform() to get the output label from the output vector\n",
        "\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(X_test[20],)\n",
        "plt.show()\n",
        "## Complete the code to predict the test data using the final model selected\n",
        "print('Predicted Label', enc.inverse_transform(________.predict((X_test_normalized[59].reshape(1,64,64,3)))))  # reshaping the input image as we are only trying to predict using a single image\n",
        "print('True Label', enc.inverse_transform(y_test_encoded)[20])                                              # using inverse_transform() to get the output label from the output vector\n",
        "\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(X_test[5])\n",
        "plt.show()\n",
        "## Complete the code to predict the test data using the final model selected\n",
        "print('Predicted Label', enc.inverse_transform(______.predict((X_test_normalized[36].reshape(1,64,64,3)))))  # reshaping the input image as we are only trying to predict using a single image\n",
        "print('True Label', enc.inverse_transform(y_test_encoded)[5])                                              # using inverse_transform() to get the output label from the output vector"
      ],
      "metadata": {
        "id": "h6toRZhP9XPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actionable Insights and Business Recommendations"
      ],
      "metadata": {
        "id": "S4JLebxA9yK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*"
      ],
      "metadata": {
        "id": "uK6hTesk90kF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____"
      ],
      "metadata": {
        "id": "aI8ZTAPd93Hc"
      }
    }
  ]
}