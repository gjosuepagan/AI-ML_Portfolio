{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1pdE8SUnl1SV",
        "_rJ1l5dnmCsj",
        "czBeIn2NpNYe",
        "BBxsMno6Ge-h",
        "sQWUKgD3ID2I",
        "OUOUHZtmrGTz",
        "T2wQPeC8rOiL",
        "tpHorlfRrg31",
        "-BCGlEiYrmO0",
        "JtjaJ2MbrrnN",
        "ymFimaJztvnw",
        "wyP7SkuPtzqx",
        "o80FY9iTt496",
        "GJc6KTf0xEfw",
        "Guu5b9VAuX0X",
        "1aIBrfdgyj93",
        "dkiWH1bvQ-PW",
        "9bjkXrvYRVA1",
        "4xhjcRRzSDSb",
        "OApp-NWv0YQJ",
        "Pmwydoc80j8Q",
        "d0stwBSSYQla",
        "u98rI_8c0ye8",
        "Pi4VjR4o1Bh-",
        "9cqurtMf6GIk",
        "CWzFRmnMJ2a0",
        "jIirR4DSKXHT",
        "_B9SBWBp58Rx",
        "81Lwi24a2O-B",
        "JGG2Qqkbjg5b",
        "TXmoGTuW6qZ0",
        "_xZg30M-2cqM",
        "1HINKccB246-",
        "_zVGnZsTgMuX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoneIOQBEVsS"
      },
      "source": [
        "# Introduction to Computer Vision: E-Commerce Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement"
      ],
      "metadata": {
        "id": "1pdE8SUnl1SV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Context"
      ],
      "metadata": {
        "id": "_rJ1l5dnmCsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Product categorization also referred to as product classification, is a field of study within natural language processing (NLP). It is also one of the biggest challenges for e-commerce companies. With the advancement of AI technology, researchers have been applying machine learning to product categorization problems.\n",
        "\n",
        "Product categorization is the placement and organization of products into their respective categories. In that sense, it sounds simple: choose the correct department for a product. However, this process is complicated by the sheer volume of products on many e-commerce platforms. Furthermore, many products could belong to multiple categories.\n",
        "\n",
        "There are many reasons why product categorization is important for e-commerce and marketing. Through the accurate classification of your products, you can increase conversion rates, strengthen your search engine, and improve your siteâ€™s Google ranking."
      ],
      "metadata": {
        "id": "gZsMRCh4mHcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective"
      ],
      "metadata": {
        "id": "czBeIn2NpNYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of the project is to build a classification model that can accurately classify product descriptions into the predefined categories of \"Household,\" \"Clothing & Accessories,\" \"Electronics,\" and \"Books.\""
      ],
      "metadata": {
        "id": "_gDmE4d8mTBC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBxsMno6Ge-h"
      },
      "source": [
        "### Data Dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Label - Class name\n",
        "* Text - Description from the e-commerce website  \n",
        "\n"
      ],
      "metadata": {
        "id": "l4_yfg-IpRkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Please read the instructions carefully before starting the project.**\n",
        "\n",
        "This is a commented Python Notebook file in which all the instructions and tasks to be performed are mentioned.\n",
        "\n",
        "* Blanks '_______' are provided in the notebook that need to be filled with an appropriate code to get the correct result\n",
        "\n",
        "* With every '_______' blank, there is a comment that briefly describes what needs to be filled in the blank space\n",
        "\n",
        "* Identify the task to be performed correctly and only then proceed to write the required code\n",
        "\n",
        "* Fill the code wherever asked by the commented lines like \"# write your code here\" or \"# complete the code\"\n",
        "\n",
        "* Running incomplete code may throw an error\n",
        "\n",
        "* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors\n",
        "\n",
        "* Add the results/observations derived from the analysis in the presentation and submit the same in .pdf format"
      ],
      "metadata": {
        "id": "kIofOc4Eq7n5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQWUKgD3ID2I"
      },
      "source": [
        "## Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxPilyLmHy_n"
      },
      "source": [
        "# install and import necessary libraries.\n",
        "\n",
        "!pip install contractions\n",
        "\n",
        "import re, string, unicodedata                          # Import Regex, string and unicodedata.\n",
        "import contractions                                     # Import contractions library.\n",
        "from bs4 import BeautifulSoup                           # Import BeautifulSoup.\n",
        "\n",
        "import numpy as np                                      # Import numpy.\n",
        "import pandas as pd                                     # Import pandas.\n",
        "import nltk                                             # Import Natural Language Tool-Kit.\n",
        "import seaborn as sns                                   # Import seaborn\n",
        "import matplotlib.pyplot as plt                         # Import Matplotlib\n",
        "\n",
        "nltk.download('stopwords')                              # Download Stopwords.\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords                       # Import stopwords.\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize  # Import Tokenizer.\n",
        "from nltk.stem.wordnet import WordNetLemmatizer         # Import Lemmatizer.\n",
        "from wordcloud import WordCloud,STOPWORDS               # Import WorldCloud and Stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer # Import count Vectorizer\n",
        "from sklearn.model_selection import train_test_split    # Import train test split\n",
        "from sklearn.ensemble import RandomForestClassifier     # Import Rndom Forest Classifier\n",
        "from sklearn.model_selection import cross_val_score     # Import cross val score\n",
        "from sklearn.metrics import confusion_matrix            # Import confusion matrix\n",
        "import wordcloud                         # Import Word Cloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Import Tf-Idf vector\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "from wordcloud import WordCloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset"
      ],
      "metadata": {
        "id": "OUOUHZtmrGTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google drive to access the dataset  (Run this code, if you are using Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "L3_jlaqurJU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meW1BElXEEft"
      },
      "source": [
        "data = pd.read_csv(\"________\")               # Complete the code to read the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Overview"
      ],
      "metadata": {
        "id": "T2wQPeC8rOiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial steps to get an overview of any dataset is to:\n",
        "- Observe the first few rows of the dataset, to check whether the dataset has been loaded properly or not\n",
        "- Get information about the number of rows and columns in the dataset\n",
        "- Find out the data types of the columns to ensure that data is stored in the preferred format and the value of each property is as expected."
      ],
      "metadata": {
        "id": "iwB5PAWbremx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the head and tail of the data"
      ],
      "metadata": {
        "id": "tpHorlfRrg31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.__________()                            # Complete the code to display the first 5 rows of the dataset\n",
        "data.__________()                            # Complete the code to display the last 5 rows of the dataset"
      ],
      "metadata": {
        "id": "tMpI-SlXriZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understand the shape of the dataset"
      ],
      "metadata": {
        "id": "-BCGlEiYrmO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.___________()                           # Complete the code to get the shape of data"
      ],
      "metadata": {
        "id": "PndM6gyZroQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking for Missing Values"
      ],
      "metadata": {
        "id": "JtjaJ2MbrrnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.'_______'                               # Complete the code to check duplicate entries in the data"
      ],
      "metadata": {
        "id": "TXpwQAVnrtRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "ymFimaJztvnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Univariate Analysis"
      ],
      "metadata": {
        "id": "wyP7SkuPtzqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create labeled barplots\n",
        "\n",
        "\n",
        "def labeled_barplot(data, feature, perc=False, n=None):\n",
        "    \"\"\"\n",
        "    Barplot with percentage at the top\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    perc: whether to display percentages instead of count (default is False)\n",
        "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
        "    \"\"\"\n",
        "\n",
        "    total = len(data[feature])  # length of the column\n",
        "    count = data[feature].nunique()\n",
        "    if n is None:\n",
        "        plt.figure(figsize=(count + 1, 5))\n",
        "    else:\n",
        "        plt.figure(figsize=(n + 1, 5))\n",
        "\n",
        "    plt.xticks(rotation=90, fontsize=15)\n",
        "    ax = sns.countplot(\n",
        "        data=data,\n",
        "        x=feature,\n",
        "        palette=\"Paired\",\n",
        "        order=data[feature].value_counts().index[:n].sort_values(),\n",
        "    )\n",
        "\n",
        "    for p in ax.patches:\n",
        "        if perc == True:\n",
        "            label = \"{:.1f}%\".format(\n",
        "                100 * p.get_height() / total\n",
        "            )  # percentage of each class of the category\n",
        "        else:\n",
        "            label = p.get_height()  # count of each level of the category\n",
        "\n",
        "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
        "        y = p.get_height()  # height of the plot\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            (x, y),\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            size=12,\n",
        "            xytext=(0, 5),\n",
        "            textcoords=\"offset points\",\n",
        "        )  # annotate the percentage\n",
        "\n",
        "    plt.show()  # show the plot"
      ],
      "metadata": {
        "id": "FzVvFqyjt1Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Percentage of text for each label"
      ],
      "metadata": {
        "id": "o80FY9iTt496"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_barplot(data, \"_________\", perc=True)         # Complete the code to plot the labeled barplot for Label"
      ],
      "metadata": {
        "id": "u30mVsMRuDdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bivariate Analysis"
      ],
      "metadata": {
        "id": "GJc6KTf0xEfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Wordcloud for each product category"
      ],
      "metadata": {
        "id": "Guu5b9VAuX0X"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3r0ddKtY5Ku"
      },
      "source": [
        "import wordcloud\n",
        "def show_wordcloud(data, title):\n",
        "    text = ' '.join(data['Text'].astype(str).tolist())                                         # Converting Summary column into list\n",
        "    stopwords = set(wordcloud.STOPWORDS)                                                       # instantiate the stopwords from wordcloud\n",
        "\n",
        "    fig_wordcloud = wordcloud.WordCloud(stopwords=stopwords,background_color='white',          # Setting the different parameter of stopwords\n",
        "                    colormap='viridis', width=800, height=600).generate(text)\n",
        "\n",
        "    plt.figure(figsize=(14,11), frameon=True)\n",
        "    plt.imshow(fig_wordcloud)\n",
        "    plt.axis('off')\n",
        "    plt.title(title, fontsize=30)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Household product**"
      ],
      "metadata": {
        "id": "2t1vAEtfwwjS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khWD_129YWq1"
      },
      "source": [
        "show_wordcloud(data[data.Label=='_________'],'Summary Word_Cloud')    # Complete the code to get the word cloud for Household"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clothing & Accessories product**"
      ],
      "metadata": {
        "id": "bkOwwO08wz5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_wordcloud(data[data.Label=='_________'],'Summary Word_Cloud')    # Complete the code to get the word cloud for Clothing & Accessories"
      ],
      "metadata": {
        "id": "qB3jmZJNwfvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Electronics product**"
      ],
      "metadata": {
        "id": "RZZdFJkSw21X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_wordcloud(data[data.Label=='_________'],'Summary Word_Cloud')    # Complete the code to get the word cloud for Electronics"
      ],
      "metadata": {
        "id": "rIO_j5SjwkhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Books product**"
      ],
      "metadata": {
        "id": "el4BLNU-w9MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_wordcloud(data[data.Label=='_________'],'Summary Word_Cloud')    # Complete the code to get the word cloud for Books"
      ],
      "metadata": {
        "id": "gQLWIalhwpfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation for Modeling"
      ],
      "metadata": {
        "id": "1aIBrfdgyj93"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_btz7Z_eQUdT"
      },
      "source": [
        "-\tHtml tag removal.\n",
        "-\tRemove the numbers.\n",
        "-\tTokenization.\n",
        "-\tRemoval of Special Characters and Punctuations.\n",
        "-\tConversion to lowercase.\n",
        "-\tLemmatize or stemming.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkiWH1bvQ-PW"
      },
      "source": [
        "### Remove HTML Tages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSO9UoltOYqE"
      },
      "source": [
        "# Code to remove the html tage\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "data['text'] = data['___'].apply(____________)                        # Complete the code to apply strip html function on text column\n",
        "data._______                                                          # Complete the code to display the head of the data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bjkXrvYRVA1"
      },
      "source": [
        "### Remove numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhD0xhXJRJcL"
      },
      "source": [
        "def remove_numbers(text):\n",
        "  text = re.sub(r'\\d+', '', text)\n",
        "  return text\n",
        "\n",
        "data['Text'] = data['_____'].apply(lambda x: remove_numbers(x))       # Complete the code to apply the above function on text column\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xhjcRRzSDSb"
      },
      "source": [
        "### Apply Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH5TCUm_RqW-"
      },
      "source": [
        "data['_____'] = data.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1) # Complete the code to apply tokenization on text column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying lowercase and removing stopwords and punctuation"
      ],
      "metadata": {
        "id": "OApp-NWv0YQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All the preprocessing steps in one function**"
      ],
      "metadata": {
        "id": "-XMuqlQe0bcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "0gF5nvihhnrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fseT_EGiSqCj"
      },
      "source": [
        "def remove_non_ascii(words):\n",
        "    #Remove non-ASCII characters from list of tokenized words\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    #Remove punctuation from list of tokenized words\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    #Convert all characters to lowercase from list of tokenized words\n",
        "    new_words = []                                  # Create empty list to store pre-processed words.\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    #Remove stop words from list of tokenized words\n",
        "    new_words = []                                  # Create empty list to store pre-processed words.\n",
        "    for word in words:\n",
        "        if word not in stopwords:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "\n",
        "def lemmatize_list(words):\n",
        "    #Lemmatize words in list of tokenized words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []                                    # Create empty list to store pre-processed words.\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word)\n",
        "        lemmas.append(lemma)                       # Append processed words to new list.\n",
        "    return lemmas\n",
        "\n",
        "\n",
        "def normalize(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = remove_stopwords(words)\n",
        "    words = lemmatize_list(words)\n",
        "    return ' '.join(words)\n",
        "\n",
        "data['Text'] = data.apply(lambda row: normalize(row['Text']), axis=1)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building"
      ],
      "metadata": {
        "id": "Pmwydoc80j8Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0stwBSSYQla"
      },
      "source": [
        "### Using countvectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization (Convert text data to numbers).\n",
        "\n",
        "Count_vec = ______________(max_features=_____)                # Complete the code to initialize the CountVectorizer function with max_ features = 5000.\n",
        "data_features = Count_vec._____(data['_____'])                # Complete the code to fit and transrofm the count_vec variable on the text column\n",
        "\n",
        "data_features = data_features._______()                       # Complete the code to convert the datafram into array"
      ],
      "metadata": {
        "id": "8ComTFfR0rFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byo-VxezY8Jy"
      },
      "source": [
        "data_features.___________                                     # Complete the code to check the shape of the data features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Store Independent and Dependent variables"
      ],
      "metadata": {
        "id": "u98rI_8c0ye8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgP4uw2DY8z6"
      },
      "source": [
        "X = _____________                                             # Complete the code to get the independent variable (data_features) stored as X\n",
        "\n",
        "y = data.__________                                           # Complete the code to get the dependent variable (Label) stored as Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split the data into train and test"
      ],
      "metadata": {
        "id": "Pi4VjR4o1Bh-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbMc1fkbf2kE"
      },
      "source": [
        "# Split data into training and testing set.\n",
        "\n",
        "X_train, X_test, y_train, y_test =_________ (__, __, test_size=___, random_state=42)   # Complete the code to split the X and Y into train and test dat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cqurtMf6GIk"
      },
      "source": [
        "#### Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Random Forest to build model for the classification of reviews.\n",
        "\n",
        "forest = ____________(n_estimators=____, n_jobs=4)            # Initialize the Random Forest Classifier\n",
        "\n",
        "forest = ______.____(______, _______)                         # Fit the forest variable on X_train and y_train\n",
        "\n",
        "print(forest)\n",
        "\n",
        "print(np.mean(_______________(forest, X, y, cv=10)))          # Calculate cross validation score"
      ],
      "metadata": {
        "id": "f3OicEYW1JC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWzFRmnMJ2a0"
      },
      "source": [
        "##### Optimize the parameter: The number of trees in the random forest model(n_estimators)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDjWjOIAJ9n0"
      },
      "source": [
        "# Finding optimal number of base learners using k-fold CV ->\n",
        "base_ln = [x for x in range(1, 25)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Fold Cross - validation .\n",
        "cv_scores = []                                                                             # Initializing a emptry list to store the score\n",
        "for b in base_ln:\n",
        "    clf = _______________(n_estimators = b)                                                # Complete the code to apply Rondome Forest Classifier\n",
        "    scores = ___________(_____, ______, _______, cv = 5, scoring = '___________')          # Complete the code to find the cross-validation score on the classifier (clf) for accuracy\n",
        "    cv_scores.append(scores.mean())                                                        # Append the scores to cv_scores list"
      ],
      "metadata": {
        "id": "0-cRt9Mc1RgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb0AoHfiKGqL"
      },
      "source": [
        "# plot the error as k increases\n",
        "error = [1 - x for x in cv_scores]                                 # Error corresponds to each number of estimator\n",
        "optimal_learners = base_ln[error.index(min(error))]                # Selection of optimal number of n_estimator corresponds to minimum error.\n",
        "plt.plot(base_ln, error)                                           # Plot between each number of estimator and misclassification error\n",
        "xy = (optimal_learners, min(error))\n",
        "plt.annotate('(%s, %s)' % xy, xy = xy, textcoords='data')\n",
        "plt.xlabel(\"Number of base learners\")\n",
        "plt.ylabel(\"Misclassification Error\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSkZ40IaKKcr"
      },
      "source": [
        "# Train the best model and calculating accuracy on test data .\n",
        "clf = _________(n_estimators = _____________)                     # Initialize the Random Forest classifier with optimal learners\n",
        "___.____(____, ___)                                               # Fit the classifer on X_train and y_train\n",
        "___.____(____, ___)                                               # Find the score on X_train and y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrEiS5LshTFX"
      },
      "source": [
        "  # Predict the result for test data using the model built above.\n",
        "  result = _____.predict(_______)                                   # Complete the code to predict the X_test data using the model built above (forest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbQPcmjshxbL"
      },
      "source": [
        "# Print and plot Confusion matirx\n",
        "\n",
        "conf_mat = ________(___________, _________)                       # Complete the code to calculate the confusion matrix between test data and result\n",
        "\n",
        "print(conf_mat)                                                   # Print confusion matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIirR4DSKXHT"
      },
      "source": [
        "#### Wordcloud of top 40 important features from countvectorizer+Randomforest based mode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_features = Count_vec.get_feature_names()                     # Instantiate the feature from the vectorizer\n",
        "top_features=''                                                  # Addition of top 40 feature into top_feature after training the model\n",
        "feat=clf.feature_importances_\n",
        "features=np.argsort(feat)[::-1]\n",
        "for i in features[0:40]:\n",
        "    top_features+=all_features[i]\n",
        "    top_features+=','\n",
        "\n",
        "print(top_features)\n",
        "\n",
        "print(\" \")\n",
        "print(\" \")\n",
        "\n",
        "# Complete the code by applying wordcloud on top features\n",
        "wordcloud = ________(background_color=\"white\",colormap='viridis',width=2000,height=1000).generate(_______)"
      ],
      "metadata": {
        "id": "t3rbi5Xg1svU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the generated image:\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.figure(1, figsize=(14, 11), frameon='equal')\n",
        "plt.title('Top 40 features WordCloud', fontsize=20)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IwVWXTZo1xHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B9SBWBp58Rx"
      },
      "source": [
        "### Using TF-IDF (Term Frequency- Inverse Document Frequency)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using TfidfVectorizer to convert text data to numbers.\n",
        "\n",
        "tfidf_vect = ______________(max_features=_____)                          # Complete the code to initialize the TF-IDF vector function with max_features = 5000.\n",
        "data_features = tfidf_vect.fit_transform(data['text'])                   # Fit the tf idf function on the text column\n",
        "\n",
        "data_features = data_features._______()                                  # Complete the code to convert the datafram into array"
      ],
      "metadata": {
        "id": "x-5XQCVY2I7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_features.___________                                                # Complete the code to check the shape of the data features"
      ],
      "metadata": {
        "id": "UT0uc_y92M15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Store Independent and Dependent variables"
      ],
      "metadata": {
        "id": "81Lwi24a2O-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = _____________                                                        # Complete the code to get the independent variable (data_features) stored as X\n",
        "\n",
        "y = data.__________                                                      # Complete the code to get the dependent variable (Label) stored as Y"
      ],
      "metadata": {
        "id": "BGrtvkgK2Q6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split the data into train and test"
      ],
      "metadata": {
        "id": "JGG2Qqkbjg5b"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK5pIhaSkcQW"
      },
      "source": [
        "# Split data into training and testing set.\n",
        "\n",
        "X_train, X_test, y_train, y_test =_________ (__, __, test_size=___, random_state=____)   # Complete the code to split the X and Y into train and test dat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXmoGTuW6qZ0"
      },
      "source": [
        "#### Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Random Forest to build model for the classification of reviews.\n",
        "\n",
        "forest = ____________(n_estimators=____, n_jobs=4)            # Initialize the Random Forest Classifier\n",
        "\n",
        "forest = ______.____(______, _______)                         # Fit the forest variable on X_train and y_train\n",
        "\n",
        "print(forest)\n",
        "\n",
        "print(np.mean(_______________(forest, X, y, cv=10)))          # Calculate cross validation score"
      ],
      "metadata": {
        "id": "m3YyqODl2Yvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimize the parameter: The number of trees in the random forest model(n_estimators)"
      ],
      "metadata": {
        "id": "_xZg30M-2cqM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU9tpeINVQlx"
      },
      "source": [
        "# Finding optimal number of base learners using k-fold CV ->\n",
        "base_ln = [x for x in range(1, 25)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Fold Cross - validation .\n",
        "cv_scores = []                                                                             # Initializing a emptry list to store the score\n",
        "for b in base_ln:\n",
        "    clf = _______________(n_estimators = b)                                                # Complete the code to apply Rondome Forest Classifier\n",
        "    scores = ___________(_____, ______, _______, cv = 5, scoring = '___________')          # Complete the code to find the cross-validation score on the classifier (clf) for accuracy\n",
        "    cv_scores.append(scores.mean())                                                        # Append the scores to cv_scores list"
      ],
      "metadata": {
        "id": "xQh7rAN52f6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4bmJIvdVVfi"
      },
      "source": [
        "# plotting the error as k increases\n",
        "error = [1 - x for x in cv_scores]                                 # #rror corresponds to each nu of estimator\n",
        "optimal_learners = base_ln[error.index(min(error))]                # Selection of optimal nu of n_estimator corresponds to minimum error.\n",
        "plt.plot(base_ln, error)                                           # Plot between each nu of estimator and misclassification error\n",
        "xy = (optimal_learners, min(error))\n",
        "plt.annotate('(%s, %s)' % xy, xy = xy, textcoords='data')\n",
        "plt.xlabel(\"Number of base learners\")\n",
        "plt.ylabel(\"Misclassification Error\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot the misclassification error for each of estimators (Hint: Use the above code which is used while plotting the miscalssification error for CountVector function)**"
      ],
      "metadata": {
        "id": "jrcHrSSj2p13"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kvz2q0Rz2sTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC-QEJs5VYsB"
      },
      "source": [
        "# Train the best model and calculating accuracy on test data .\n",
        "clf = _________(n_estimators = _____________)                     # Initialize the Random Forest classifier with optimal learners\n",
        "___.____(____, ___)                                               # Fit the classifer on X_train and y_train\n",
        "___.____(____, ___)                                               # Find the score on X_train and y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX95ZsHAlIVQ"
      },
      "source": [
        "# Predict the result for test data using the model built above.\n",
        "result = _____.predict(_______)                                   # Complete the code to predict the X_test data using the model built above (forest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Wordcloud of top 40 important features from TF-IDF+Randomforest based mode"
      ],
      "metadata": {
        "id": "1HINKccB246-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPptTFrZVwMW"
      },
      "source": [
        "all_features = tfidf_vect.get_feature_names_out()          # Instantiate the feature from the vectorizer\n",
        "top_features=''                                            # Addition of top 40 feature into top_feature after training the model\n",
        "feat=clf.feature_importances_\n",
        "features=np.argsort(feat)[::-1]\n",
        "for i in features[0:40]:\n",
        "    top_features+=all_features[i]\n",
        "    top_features+=', '\n",
        "\n",
        "print(top_features)\n",
        "\n",
        "print(\" \")\n",
        "print(\" \")\n",
        "\n",
        "# Complete the code by applying wordcloud on top features\n",
        "wordcloud = ________(background_color=\"white\",colormap='viridis',width=2000,height=1000).generate(_______)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the generated image:\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.figure(1, figsize=(14, 11), frameon='equal')\n",
        "plt.title('Top 40 features WordCloud', fontsize=20)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7SluTJux3Dio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zVGnZsTgMuX"
      },
      "source": [
        "## Summary\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gd_c3NQn3N3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Happy Learning!"
      ],
      "metadata": {
        "id": "6ymbt6KR3PgO"
      }
    }
  ]
}